import OpenAI from "openai";
import type {
  AIModel,
  ChatMessage,
  AIRequest,
  AIResponse,
  MessageContent,
  RequestContext,
} from "../types/ai.js";
import { RAGService } from "./ragService.js";
import { createDatabaseAdapter } from "./databaseAdapter.js";

export class AIService {
  private openai: OpenAI;
  private ragService: RAGService;
  private models: AIModel[] = [
    {
      id: "openai/gpt-5-chat",
      name: "GPT-5",
      provider: "openai",
      description:
        "Modelo multimodal de √∫ltima gera√ß√£o da OpenAI com forte racioc√≠nio",
      capabilities: ["text", "images", "multimodal", "analysis", "reasoning"],
      maxTokens: 200000,
    },
    {
      id: "openai/gpt-4.1",
      name: "GPT-4.1",
      provider: "openai",
      description: "Modelo mais avan√ßado da OpenAI para an√°lises complexas",
      capabilities: ["text", "analysis", "reasoning"],
      maxTokens: 128000,
    },
    {
      id: "google/gemini-2.5-pro",
      name: "Gemini 2.5 Pro",
      provider: "google",
      description:
        "Modelo multimodal do Google com capacidade de an√°lise de imagens",
      capabilities: ["text", "images", "multimodal", "analysis"],
      maxTokens: 2000000,
    },
    {
      id: "google/gemini-2.5-flash",
      name: "Gemini 2.5 Flash",
      provider: "google",
      description:
        "Modelo r√°pido do Google otimizado para velocidade, ideal para resumos e an√°lises r√°pidas",
      capabilities: ["text", "images", "multimodal", "analysis", "fast"],
      maxTokens: 1000000,
    },
    {
      id: "openai/gpt-4o-mini",
      name: "GPT-4o Mini",
      provider: "openai",
      description: "Modelo r√°pido e eficiente da OpenAI, ideal para tarefas que precisam de velocidade",
      capabilities: ["text", "analysis", "fast"],
      maxTokens: 128000,
    },
    {
      id: "anthropic/claude-sonnet-4",
      name: "Claude Sonnet 4",
      provider: "anthropic",
      description: "Modelo da Anthropic focado em seguran√ßa e precis√£o",
      capabilities: ["text", "analysis", "safety", "reasoning"],
      maxTokens: 200000,
    },
    {
      id: "x-ai/grok-4",
      name: "Grok 4",
      provider: "x-ai",
      description: "Modelo open source eficiente para an√°lises r√°pidas",
      capabilities: ["text", "analysis", "efficiency"],
      maxTokens: 131072,
    },
  ];

  constructor() {
    const apiKey = process.env.OPENROUTER_API_KEY;
    const siteUrl = process.env.SITE_URL || "http://localhost:3000";
    const siteName = process.env.SITE_NAME || "CondoGov AdminAssistant";

    if (!apiKey) {
      throw new Error("OPENROUTER_API_KEY environment variable is required");
    }

    this.openai = new OpenAI({
      baseURL: "https://openrouter.ai/api/v1",
      apiKey: apiKey,
      defaultHeaders: {
        "HTTP-Referer": siteUrl,
        "X-Title": siteName,
      },
    });

    // Inicializar RAG Service
    const databaseAdapter = createDatabaseAdapter();
    this.ragService = new RAGService(databaseAdapter);
  }

  getModels(): AIModel[] {
    return this.models;
  }

  getModel(modelId: string): AIModel | undefined {
    return this.models.find((model) => model.id === modelId);
  }

  async sendMessageWithRAG(
    request: AIRequest,
    context: RequestContext
  ): Promise<AIResponse> {
    try {
      const model = this.getModel(request.model);
      if (!model) {
        throw new Error(`Model ${request.model} not found`);
      }

      // Buscar conhecimento relevante usando RAG
      const ragResult = await this.ragService.retrieveKnowledge(
        request.message,
        context
      );

      // Montar prompt enriquecido
      const enrichedPrompt = this.ragService.buildEnrichedPrompt(
        request.message,
        ragResult.citations,
        ragResult.memories,
        context
      );

      // Preparar mensagens para o OpenAI
      const messages = this.prepareMessagesWithRAG(enrichedPrompt, context);

      const completion = await this.openai.chat.completions.create({
        model: request.model,
        messages: messages,
        temperature: 0.7,
        max_tokens: 4096,
      });

      const response = completion.choices[0]?.message;
      if (!response) {
        throw new Error("No response from AI model");
      }

      // Extrair mem√≥rias da conversa para aprendizado futuro
      await this.ragService.extractMemories(
        request.message,
        response.content || "",
        context
      );

      return {
        message: response.content || "",
        model: request.model,
        tokens: completion.usage?.total_tokens || 0,
        sessionId: request.sessionId || this.generateSessionId(),
        messageId: this.generateMessageId(),
        timestamp: new Date(),
        citations: ragResult.citations,
        memoryUsed: ragResult.memories,
      };
    } catch (error) {
      console.error("Error sending message with RAG:", error);
      throw new Error(
        `Failed to get AI response: ${
          error instanceof Error ? error.message : "Unknown error"
        }`
      );
    }
  }

  async sendMessage(request: AIRequest): Promise<AIResponse> {
    try {
      const model = this.getModel(request.model);
      if (!model) {
        throw new Error(`Model ${request.model} not found`);
      }

      // Preparar mensagens para o OpenAI
      const messages = this.prepareMessages(request);

      const completion = await this.openai.chat.completions.create({
        model: request.model,
        messages: messages,
        temperature: 0.7,
        max_tokens: 4096,
      });

      const response = completion.choices[0]?.message;
      if (!response) {
        throw new Error("No response from AI model");
      }

      return {
        message: response.content || "",
        model: request.model,
        tokens: completion.usage?.total_tokens || 0,
        sessionId: request.sessionId || this.generateSessionId(),
        messageId: this.generateMessageId(),
        timestamp: new Date(),
      };
    } catch (error) {
      console.error("Error sending message to AI:", error);
      throw new Error(
        `Failed to get AI response: ${
          error instanceof Error ? error.message : "Unknown error"
        }`
      );
    }
  }

  async sendMessageWithImages(
    request: AIRequest,
    imageUrls: string[]
  ): Promise<AIResponse> {
    try {
      const model = this.getModel(request.model);
      if (!model) {
        throw new Error(`Model ${request.model} not found`);
      }

      // Verificar se o modelo suporta imagens
      if (
        !model.capabilities.includes("images") &&
        !model.capabilities.includes("multimodal")
      ) {
        throw new Error(
          `Model ${request.model} does not support image analysis`
        );
      }

      // Preparar conte√∫do com imagens usando tipos compat√≠veis com OpenAI
      const content: OpenAI.Chat.Completions.ChatCompletionContentPart[] = [
        {
          type: "text",
          text: request.message,
        },
      ];

      // Adicionar imagens
      imageUrls.forEach((url) => {
        content.push({
          type: "image_url",
          image_url: { url },
        });
      });

      const messages: OpenAI.Chat.Completions.ChatCompletionMessageParam[] = [
        {
          role: "user",
          content: content,
        },
      ];

      const completion = await this.openai.chat.completions.create({
        model: request.model,
        messages: messages,
        temperature: 0.7,
        max_tokens: 4096,
      });

      const response = completion.choices[0]?.message;
      if (!response) {
        throw new Error("No response from AI model");
      }

      return {
        message: response.content || "",
        model: request.model,
        tokens: completion.usage?.total_tokens || 0,
        sessionId: request.sessionId || this.generateSessionId(),
        messageId: this.generateMessageId(),
        timestamp: new Date(),
      };
    } catch (error) {
      console.error("Error sending message with images to AI:", error);
      throw new Error(
        `Failed to get AI response: ${
          error instanceof Error ? error.message : "Unknown error"
        }`
      );
    }
  }

  async analyzeCondominiumData(
    data: any,
    analysisType: string,
    model: string = "openai/gpt-4.1",
    userId: string = "system"
  ): Promise<AIResponse> {
    const prompts = {
      performance: `Analise os dados de performance deste condom√≠nio e forne√ßa insights sobre:
      - Taxa de conclus√£o de projetos
      - Projetos atrasados e suas causas
      - Efici√™ncia operacional
      - Recomenda√ß√µes para melhorias
      
      Dados: ${JSON.stringify(data, null, 2)}`,

      financial: `Analise a situa√ß√£o financeira deste condom√≠nio e forne√ßa:
      - An√°lise de receitas e despesas
      - Previs√µes financeiras
      - Identifica√ß√£o de problemas financeiros
      - Sugest√µes de otimiza√ß√£o de or√ßamento
      
      Dados: ${JSON.stringify(data, null, 2)}`,

      alerts: `Analise os alertas cr√≠ticos deste condom√≠nio e priorize:
      - Quest√µes mais urgentes
      - Impacto potencial de cada alerta
      - Plano de a√ß√£o recomendado
      - Preven√ß√£o de problemas futuros
      
      Dados: ${JSON.stringify(data, null, 2)}`,

      optimization: `Analise os processos deste condom√≠nio e sugira otimiza√ß√µes:
      - Processos que podem ser automatizados
      - Melhorias na gest√£o
      - Redu√ß√£o de custos operacionais
      - Aumento da satisfa√ß√£o dos moradores
      
      Dados: ${JSON.stringify(data, null, 2)}`,
    };

    const prompt =
      prompts[analysisType as keyof typeof prompts] || prompts.performance;

    return this.sendMessage({
      message: prompt,
      model: model,
      userId: userId,
      context: { analysisType, data },
    });
  }

  private prepareMessagesWithRAG(
    enrichedPrompt: string,
    context: RequestContext
  ): OpenAI.Chat.Completions.ChatCompletionMessageParam[] {
    const systemMessage: OpenAI.Chat.Completions.ChatCompletionMessageParam = {
      role: "system",
      content: this.getSystemPrompt(context),
    };

    const userMessage: OpenAI.Chat.Completions.ChatCompletionMessageParam = {
      role: "user",
      content: enrichedPrompt,
    };

    return [systemMessage, userMessage];
  }

  private prepareMessages(
    request: AIRequest,
    context?: RequestContext
  ): OpenAI.Chat.Completions.ChatCompletionMessageParam[] {
    // Usar getSystemPrompt se tiver contexto, sen√£o usar prompt padr√£o
    const systemContent = context 
      ? this.getSystemPrompt(context)
      : `Voc√™ √© o AdminAssistantAI, um assistente inteligente especializado em gest√£o de condom√≠nios. 
      Voc√™ ajuda administradores com an√°lises, insights e suporte para tomada de decis√µes.
      
      Suas especialidades incluem:
      - An√°lise de performance de projetos
      - Gest√£o financeira de condom√≠nios
      - Identifica√ß√£o e prioriza√ß√£o de alertas
      - Otimiza√ß√£o de processos
      - Suporte √† tomada de decis√µes
      
      Sempre forne√ßa respostas pr√°ticas, objetivas e acion√°veis.`;

    const systemMessage: OpenAI.Chat.Completions.ChatCompletionMessageParam = {
      role: "system",
      content: systemContent,
    };

    const userMessage: OpenAI.Chat.Completions.ChatCompletionMessageParam = {
      role: "user",
      content: request.message,
    };

    return [systemMessage, userMessage];
  }

  private getSystemPrompt(context: RequestContext): string {
    const basePrompt = `Voc√™ √© o AdminAssistantAI, um assistente inteligente especializado em gest√£o condominial da plataforma CondoGov Connect.

CONTEXTO ATUAL:
- Usu√°rio: ${context.userId}
- Modo: ${context.contextMode}${context.sector ? ` (Setor: ${context.sector})` : ""}

SUAS CAPACIDADES:
- An√°lise de dados com base no conhecimento da empresa
- Personaliza√ß√£o baseada no hist√≥rico do usu√°rio
- Suporte especializado por setor quando aplic√°vel
- Respostas fundamentadas em informa√ß√µes verificadas

INSTRU√á√ïES ESPEC√çFICAS:
- Use sempre as informa√ß√µes fornecidas no contexto
- Cite fontes quando relevante
- Personalize respostas com base nas mem√≥rias do usu√°rio
- Mantenha foco no setor espec√≠fico quando em modo setorial
- Seja pr√°tico, objetivo e acion√°vel
- Se n√£o souber algo, seja honesto sobre as limita√ß√µes
- IMPORTANTE: NUNCA mencione IDs t√©cnicos (como UUIDs de empresa, usu√°rio ou sistema) nas suas respostas. Use apenas informa√ß√µes descritivas e nomes quando dispon√≠veis.

${context.contextMode === "sector" && context.sector ? 
  `FOCO SETORIAL: Suas respostas devem priorizar informa√ß√µes e a√ß√µes relacionadas ao setor ${context.sector}.` : 
  `VIS√ÉO GERAL: Forne√ßa uma perspectiva ampla da empresa, considerando todos os setores relevantes.`
}`;

    return basePrompt;
  }

  private generateSessionId(): string {
    // Usar crypto.randomUUID() para gerar UUID v√°lido
    if (typeof crypto !== 'undefined' && crypto.randomUUID) {
      return crypto.randomUUID();
    }
    // Fallback: gerar UUID v4 manualmente
    return 'xxxxxxxx-xxxx-4xxx-yxxx-xxxxxxxxxxxx'.replace(/[xy]/g, function(c) {
      const r = Math.random() * 16 | 0;
      const v = c === 'x' ? r : (r & 0x3 | 0x8);
      return v.toString(16);
    });
  }

  private generateMessageId(): string {
    // Usar UUID para compatibilidade com banco de dados
    if (typeof crypto !== 'undefined' && crypto.randomUUID) {
      return crypto.randomUUID();
    }
    // Fallback: gerar UUID v4 manualmente
    return 'xxxxxxxx-xxxx-4xxx-yxxx-xxxxxxxxxxxx'.replace(/[xy]/g, function(c) {
      const r = Math.random() * 16 | 0;
      const v = c === 'x' ? r : (r & 0x3 | 0x8);
      return v.toString(16);
    });
  }

  /**
   * Gera um resumo r√°pido e direto para um setor espec√≠fico com alertas cr√≠ticos
   * Ideal para exibi√ß√£o inicial no dashboard
   */
  async generateQuickSectorSummary(
    sector: string,
    context: RequestContext,
    model: string = "google/gemini-2.5-flash"
  ): Promise<AIResponse & { criticalAlerts?: any[]; quickTip?: string }> {
    try {
      const modelInfo = this.getModel(model);
      if (!modelInfo) {
        throw new Error(`Model ${model} not found`);
      }

      // Buscar dados do setor usando RAG (limitado para acelerar)
      const sectorQuery = `resumo r√°pido do setor ${sector} alertas cr√≠ticos urgente`;
      const ragResult = await this.ragService.retrieveKnowledge(sectorQuery, {
        ...context,
        contextMode: "sector",
        sector: sector,
      });

      // Limitar cita√ß√µes para acelerar (m√°ximo 5)
      if (ragResult.citations && ragResult.citations.length > 5) {
        ragResult.citations = ragResult.citations.slice(0, 5);
      }

      // Extrair alertas cr√≠ticos das cita√ß√µes
      const criticalAlerts = this.extractCriticalAlerts(ragResult.citations || [], sector);

      // Montar prompt para resumo r√°pido
      const enrichedPrompt = this.ragService.buildEnrichedPrompt(
        `Gere um resumo R√ÅPIDO e DIRETO para o setor ${sector}. Seja CONCISO (m√°ximo 250 palavras).

INCLUA APENAS:
1. üìä RESUMO EXECUTIVO: 2-3 frases sobre a situa√ß√£o atual
2. ‚ö†Ô∏è ALERTAS CR√çTICOS: Apenas itens que precisam de a√ß√£o IMEDIATA (se houver)
3. üí° DICA R√ÅPIDA: Uma dica pr√°tica e acion√°vel para hoje

N√ÉO inclua:
- Detalhes extensos
- Recomenda√ß√µes gerais
- Pr√≥ximos passos de longo prazo

Seja DIRETO e OBJETIVO. Use os dados fornecidos.`,
        ragResult.citations,
        ragResult.memories,
        {
          ...context,
          contextMode: "sector",
          sector: sector,
        }
      );

      const systemPrompt = `Voc√™ √© um assistente especializado em gest√£o condominial.

Voc√™ est√° gerando um RESUMO R√ÅPIDO para o setor ${sector} que ser√° exibido em um dashboard.

REGRAS:
- Seja EXTREMAMENTE CONCISO (m√°ximo 250 palavras)
- Foque apenas no ESSENCIAL
- Destaque APENAS alertas cr√≠ticos que precisam de a√ß√£o imediata
- Use emojis para melhorar a legibilidade (üìä, ‚ö†Ô∏è, üí°)
- NUNCA mencione IDs t√©cnicos
- Formato: Markdown simples com se√ß√µes curtas

FORMATO:
## üìä Resumo Executivo
[2-3 frases sobre a situa√ß√£o]

## ‚ö†Ô∏è Alertas Cr√≠ticos
[Se houver, apenas itens urgentes. Se n√£o houver, diga "‚úÖ Nenhum alerta cr√≠tico no momento."]

## üí° Dica R√°pida
[Uma dica pr√°tica e acion√°vel para hoje]`;

      const messages: OpenAI.Chat.Completions.ChatCompletionMessageParam[] = [
        {
          role: "system",
          content: systemPrompt,
        },
        {
          role: "user",
          content: enrichedPrompt,
        },
      ];

      console.log(`[AIService] Gerando resumo r√°pido para setor: ${sector}...`);
      const startTime = Date.now();

      const isFastModel = model.includes("flash") || model.includes("mini") || model.includes("fast");
      const maxTokens = isFastModel ? 600 : 800;
      const temperature = isFastModel ? 0.5 : 0.6;

      const completion = await this.openai.chat.completions.create({
        model: model,
        messages: messages,
        temperature: temperature,
        max_tokens: maxTokens,
      });

      const elapsedTime = Date.now() - startTime;
      console.log(`[AIService] Resumo r√°pido gerado em ${elapsedTime}ms. Tokens: ${completion.usage?.total_tokens || 0}`);

      const response = completion.choices[0]?.message;
      if (!response) {
        throw new Error("No response from AI model");
      }

      // Extrair dica r√°pida do texto (√∫ltima se√ß√£o)
      const quickTip = this.extractQuickTip(response.content || "");

      return {
        message: response.content || "",
        model: model,
        tokens: completion.usage?.total_tokens || 0,
        sessionId: this.generateSessionId(),
        messageId: this.generateMessageId(),
        timestamp: new Date(),
        citations: ragResult.citations || [],
        memoryUsed: ragResult.memories || [],
        criticalAlerts: criticalAlerts,
        quickTip: quickTip,
      };
    } catch (error) {
      console.error("Error generating quick sector summary:", error);
      throw new Error(
        `Failed to generate quick sector summary: ${
          error instanceof Error ? error.message : "Unknown error"
        }`
      );
    }
  }

  /**
   * Extrai alertas cr√≠ticos das cita√ß√µes baseado em palavras-chave e prioridades
   */
  private extractCriticalAlerts(citations: any[], sector: string): any[] {
    const alerts: any[] = [];
    const criticalKeywords = ["urgente", "cr√≠tico", "cr√≠tico", "emerg√™ncia", "atrasado", "vencido", "quebrado", "falha", "erro", "problema"];
    
    citations.forEach((citation) => {
      const content = (citation.content || "").toLowerCase();
      const hasCriticalKeyword = criticalKeywords.some(keyword => content.includes(keyword));
      
      // Verificar se tem status cr√≠tico
      const hasCriticalStatus = citation.tags?.some((tag: string) => 
        ["urgent", "critical", "high", "broken", "overdue", "failed"].includes(tag.toLowerCase())
      );

      if (hasCriticalKeyword || hasCriticalStatus || citation.score > 0.9) {
        alerts.push({
          id: citation.chunkId,
          sector: citation.sector || sector,
          content: citation.content?.substring(0, 150) || "",
          priority: hasCriticalKeyword ? "critical" : "high",
          source: citation.sourceId,
          tags: citation.tags || [],
        });
      }
    });

    return alerts.slice(0, 5); // M√°ximo 5 alertas
  }

  /**
   * Extrai a dica r√°pida do texto gerado
   */
  private extractQuickTip(text: string): string {
    // Procurar pela se√ß√£o "üí° Dica R√°pida" ou "üí° Dica"
    const tipMatch = text.match(/##\s*üí°\s*Dica\s*R√°pida?\s*\n([\s\S]*?)(?=\n##|$)/i);
    if (tipMatch && tipMatch[1]) {
      return tipMatch[1].trim().substring(0, 200); // Limitar tamanho
    }
    
    // Se n√£o encontrar, procurar por qualquer se√ß√£o com üí°
    const emojiMatch = text.match(/üí°\s*([^\n]+(?:\n[^\n]+){0,2})/);
    if (emojiMatch && emojiMatch[1]) {
      return emojiMatch[1].trim().substring(0, 200);
    }

    return "";
  }

  /**
   * Gera um resumo executivo completo para um setor espec√≠fico
   * Busca dados do BD e gera automaticamente resumo, alertas, dicas e pr√≥ximos passos
   */
  async generateSectorSummary(
    sector: string,
    context: RequestContext,
    model: string = "google/gemini-2.5-flash"
  ): Promise<AIResponse> {
    try {
      const modelInfo = this.getModel(model);
      if (!modelInfo) {
        throw new Error(`Model ${model} not found`);
      }

      // Buscar dados do setor usando RAG (limitado para acelerar)
      const sectorQuery = `resumo do setor ${sector} situa√ß√£o atual alertas recomenda√ß√µes`;
      const ragResult = await this.ragService.retrieveKnowledge(sectorQuery, {
        ...context,
        contextMode: "sector",
        sector: sector,
      });

      // Limitar cita√ß√µes para acelerar o processamento (m√°ximo 10)
      if (ragResult.citations && ragResult.citations.length > 10) {
        ragResult.citations = ragResult.citations.slice(0, 10);
      }

      // Montar prompt espec√≠fico para resumo executivo do setor
      const enrichedPrompt = this.ragService.buildEnrichedPrompt(
        `Gere um resumo executivo completo e detalhado para o setor ${sector}. Inclua:
1. VIS√ÉO GERAL: Situa√ß√£o atual do setor
2. SITUA√á√ÉO ATUAL: Dados principais, estat√≠sticas e status
3. ALERTAS: Itens que precisam de aten√ß√£o urgente
4. RECOMENDA√á√ïES: O que fazer para melhorar
5. PR√ìXIMOS PASSOS: A√ß√µes pr√°ticas e priorizadas
6. DICAS: Orienta√ß√µes operacionais para a equipe

Seja detalhado, pr√°tico e acion√°vel. Use os dados fornecidos no contexto.`,
        ragResult.citations,
        ragResult.memories,
        {
          ...context,
          contextMode: "sector",
          sector: sector,
        }
      );

      // Preparar mensagens para o OpenAI
      const systemPrompt = `Voc√™ √© um assistente especializado em gest√£o condominial da plataforma CondoGov Connect.

Voc√™ est√° gerando um resumo executivo autom√°tico para o setor ${sector}.

INSTRU√á√ïES:
- Gere um resumo completo, detalhado e pr√°tico
- Use APENAS os dados fornecidos no contexto
- Seja objetivo e acion√°vel
- Organize em se√ß√µes claras (Vis√£o Geral, Situa√ß√£o Atual, Alertas, Recomenda√ß√µes, Pr√≥ximos Passos, Dicas)
- Use emojis para melhorar a legibilidade (üîç, üìä, ‚ö†Ô∏è, üí°, üß≠, etc.)
- NUNCA mencione IDs t√©cnicos (UUIDs) nas respostas
- Foque em informa√ß√µes pr√°ticas e operacionais para a equipe do setor
- Se n√£o houver dados suficientes, seja claro sobre as limita√ß√µes

FORMATO:
Use markdown com se√ß√µes bem definidas. Seja detalhado e completo.`;

      const messages: OpenAI.Chat.Completions.ChatCompletionMessageParam[] = [
        {
          role: "system",
          content: systemPrompt,
        },
        {
          role: "user",
          content: enrichedPrompt,
        },
      ];

      console.log(`[AIService] Enviando requisi√ß√£o para gerar resumo do setor ${sector}...`);
      const startTime = Date.now();

      // Usar menos tokens para modelos r√°pidos, acelerar a resposta
      const isFastModel = model.includes("flash") || model.includes("mini") || model.includes("fast");
      const maxTokens = isFastModel ? 2048 : 4096;
      const temperature = isFastModel ? 0.6 : 0.7; // Menor temperatura = mais r√°pido e determin√≠stico

      const completion = await this.openai.chat.completions.create({
        model: model,
        messages: messages,
        temperature: temperature,
        max_tokens: maxTokens,
      });

      const elapsedTime = Date.now() - startTime;
      console.log(`[AIService] Resumo gerado em ${elapsedTime}ms. Tokens: ${completion.usage?.total_tokens || 0}`);

      const response = completion.choices[0]?.message;
      if (!response) {
        throw new Error("No response from AI model");
      }

      const messageContent = response.content || "";
      console.log(`[AIService] Tamanho da resposta: ${messageContent.length} caracteres`);

      return {
        message: messageContent,
        model: model,
        tokens: completion.usage?.total_tokens || 0,
        sessionId: this.generateSessionId(),
        messageId: this.generateMessageId(),
        timestamp: new Date(),
        citations: ragResult.citations || [],
        memoryUsed: ragResult.memories || [],
      };
    } catch (error) {
      console.error("Error generating sector summary:", error);
      throw new Error(
        `Failed to generate sector summary: ${
          error instanceof Error ? error.message : "Unknown error"
        }`
      );
    }
  }

}
